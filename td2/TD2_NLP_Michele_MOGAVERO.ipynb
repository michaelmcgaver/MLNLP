{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TD2 - Machine Learning for NLP\n",
        "## Michele MOGAVERO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "GofnUCJPjYLE"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tag import UnigramTagger\n",
        "from nltk.corpus import treebank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OERNtiYm72U"
      },
      "source": [
        "We use the first 3000 tagged sentences of the treebank corpus as the training set to initialize the UnigramTagger class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k54fawukSzV",
        "outputId": "ba75e1d3-4de6-4f27-8737-616726df3e53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.'] \n",
            "\n",
            "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "train_sents = treebank.tagged_sents()[:3000]\n",
        "tagger = UnigramTagger(train_sents)\n",
        "review = treebank.sents()[0]\n",
        "print(review, '\\n')\n",
        "print(tagger.tag(treebank.sents()[0])) # After tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knGVKckT-cem"
      },
      "source": [
        "After showing how the treebank and UnigramTagger works, we use an other approch to differentiate the solutions about tagging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwhY3CVkuyWl",
        "outputId": "ffb18ae0-d32d-4e5f-c89b-b233c9c27406"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package sentiwordnet to\n",
            "[nltk_data]     C:\\Users\\Michele\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Michele\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('sentiwordnet')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "piogZLyV9piA"
      },
      "outputs": [],
      "source": [
        "review=\"Today I feel so lucky and happy!\" # test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T594XgotylI",
        "outputId": "6f938d44-ccab-449b-b2dc-8b7def72e7bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Today', 'I', 'feel', 'so', 'lucky', 'and', 'happy', '!']\n",
            "[('Today', 'NN'), ('I', 'PRP'), ('feel', 'VBP'), ('so', 'RB'), ('lucky', 'JJ'), ('and', 'CC'), ('happy', 'JJ'), ('!', '.')]\n"
          ]
        }
      ],
      "source": [
        "token = nltk.word_tokenize(review)\n",
        "after_tagging = nltk.pos_tag(token)\n",
        "print (token)\n",
        "print (after_tagging)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDrYi7_zwIub"
      },
      "source": [
        "We only keep 'RB' and 'JJ', which are respectively adverbs and adjectives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "_702dfnwzJcG"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxfotFXR_UC8"
      },
      "source": [
        "A function that converts the treebank tags to wordnet, which is more verbose. We also want to keep the NOUN and VERB because in a future development they may also be considered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "aKk-AAeo3YeC"
      },
      "outputs": [],
      "source": [
        "def treebank_to_wn(tag):\n",
        "    \"\"\"\n",
        "    Converts from TreeBank to WordNet tags\n",
        "    \"\"\"\n",
        "    if tag.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif tag.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wn.ADV\n",
        "    elif tag.startswith('V'):\n",
        "        return wn.VERB\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TID2hL39_c3_"
      },
      "source": [
        "This script takes tagged list, keeps only the ADJ and ADV, lemmatizes the word, and skip it if not found.\n",
        "After that it finds an average among the words sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEnO2okl3aNO",
        "outputId": "10a10b36-533a-441d-dc98-6cebea7807f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<so.r.01: PosScore=0.0 NegScore=0.0>\n",
            "<lucky.s.01: PosScore=0.75 NegScore=0.0>\n",
            "<happy.a.01: PosScore=0.875 NegScore=0.0>\n",
            "sentiment:  0.5416666666666666\n"
          ]
        }
      ],
      "source": [
        "sentiment = 0.0\n",
        "tokens_count = 0\n",
        "for word, tag in after_tagging:\n",
        "    wn_tag = treebank_to_wn(tag) # convert to wn tag\n",
        "    if wn_tag not in (wn.ADJ, wn.ADV):\n",
        "        continue\n",
        "\n",
        "    lemma = lemmatizer.lemmatize(word, pos=wn_tag) # lemmatization\n",
        "    if not lemma:\n",
        "        continue\n",
        "\n",
        "    synsets = wn.synsets(lemma, pos=wn_tag) # if not present in synsets\n",
        "    if not synsets:\n",
        "        continue\n",
        "\n",
        "    synset = synsets[0] # we take the most common sense\n",
        "    swn_synset = swn.senti_synset(synset.name())\n",
        "    print(swn_synset)\n",
        "    sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
        "    tokens_count += 1\n",
        "score = sentiment/tokens_count\n",
        "print('sentiment: ', score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMDcmauw-UPc"
      },
      "source": [
        "Copy of the whole function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "f4uWikAY7ZG1"
      },
      "outputs": [],
      "source": [
        "def eval_sentiment(path):\n",
        "    sentiment = 0.0\n",
        "    tokens_count = 0\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    with open(path, 'r') as f:\n",
        "        review = f.read()\n",
        "        token = nltk.word_tokenize(review)\n",
        "        after_tagging = nltk.pos_tag(token)\n",
        "        for word, tag in after_tagging:\n",
        "            wn_tag = treebank_to_wn(tag) # convert to wn tag\n",
        "            if wn_tag not in (wn.ADJ, wn.ADV):\n",
        "                continue\n",
        "            lemma = lemmatizer.lemmatize(word, pos=wn_tag) # lemmatization\n",
        "            if not lemma:\n",
        "                continue\n",
        "            synsets = wn.synsets(lemma, pos=wn_tag) # if not present in synsets\n",
        "            if not synsets:\n",
        "                continue\n",
        "        sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
        "        tokens_count += 1\n",
        "\n",
        "        print(sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "JNbRiCNO7D_g"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.875\n"
          ]
        }
      ],
      "source": [
        "pos_review_path = './txt_sentoken/pos/cv000_29590.txt'\n",
        "neg_review_path = './txt_sentoken/neg/cv000_29416.txt'\n",
        "\n",
        "eval_sentiment(pos_review_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
